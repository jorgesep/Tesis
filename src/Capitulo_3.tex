\chapter{Algoritmos sustracción del fondo}

\section{Introducción}


Esta capítulo hace una presentación de dos algoritmos basados en modelos estadísticos para describir el segundo plano de una secuencia y hacer separación de objetos en movimiento de las imágenes del fondo. Se describe el modelo de \textit{Zivkovic y Heidjen} \cite{zivkovic_efficient_2006} de Mixtura de Gaussianas, el cual constituye la base de la mejora propuesta por \textit{Zezhi Chen} \cite{chen_vehicle_2012}. Estos modelos son los algoritmos sobre los cuales se desarrolla este trabajo de tesis. Ambos han sido incluidos en el desarrollo del sistema de software que permite ejecutar, para luego  comparar y evaluar desempeño sobre las secuencias proporcionadas por MuHAVI. El algoritmo de \textit{Zivkovic y Heidjen} se denomina \textit{MOG} (``\textit{Mixture of Gaussians}'') y el de \textit{Zezhi Chen} \textit{SAGMM} (``\textit{Self-Adaptive Mixture of Gaussians}''). 




\section{Modelo General}

La idea general del proceso de sustracción de fondo, es modelar cada uno de los píxeles de una escena, mediante una función de densidad de probabilidad. Se fundamenta, en la suposición que las imágenes de una escena sin objetos en circulación, tienen un comportamiento regular que puede ser descrito por un modelo estadístico. Un evento por ejemplo, podría ser detectado identificando las partes de la imagen, que no se ajustan con el modelo que describe el fondo. Un nuevo píxel sólo va a ser considerado parte del fondo, si su valor queda completamente descrito por la función de densidad de probabilidad. 


El valor de intensidad en el tiempo de un píxel (o en el transcurso de una secuencia), es utilizado para construir una función de densidad de probabilidad, que corresponde al modelo estadístico que describe el comportamiento del píxel. El modelo, es determinado por una combinación lineal de un conjunto de distribuciones Gaussianas, y por su parte, cada componente es caracterizado por un grupo de parámetros, que identifica los elementos que emergen durante el avance de las imágenes.

\begin{figure}[h!]
  \centering
      \includegraphics[scale=0.5]{img/figura_3_1}
  \caption[Imágenes 570, 610 de secuencia ``\textit{Kick Camera 3 Person 4}'']{Imágenes 570 y 610 de la secuencia MuHAVI-MAS ``\textit{Kick Camera 3 Person 4}''. El punto en rojo señala la posición de un píxel en dos eventos diferentes}
\label{posicion_340_160}
\end{figure}

\begin{figure}[h!]
  \centering
      \includegraphics[scale=0.5]{img/figura_3_2}
  \caption[Cambios de intensidad en el tiempo del valor de un pixel ]{Cambios del valor de intensidad de un color en la secuencia completa.}
\label{intensidad_340_160}
\end{figure}


La separación de píxeles entre elementos del fondo y otros objetos, puede ser explicado en forma simple, mediante las dos imágenes de la figura \ref{posicion_340_160}, tomadas de la secuencia MuHAVI \textit{``Kick Camera 3 Person 4''}\cite{singh_muhavi_2010}. Ambas imágenes señalan el estado de un píxel (punto en rojo) en dos momentos diferentes. El píxel indicado en la imagen de la figura \ref{posicion_340_160}(a), es parte en ese instante del actor (\textit{foreground}), ubicado en medio del escenario, y su valor queda determinado por los colores de su vestimenta. Unos segundos después el actor se encuentra en una posición diferente, realizando una determinada acción. En este nuevo instante el valor del píxel queda definido por los colores del escenario (\textit{background}). La gráfica de la secuencia completa (730 imágenes), que describe el comportamiento del píxel se muestra en la figura \ref{intensidad_340_160}. Los cambios repentinos de intensidad (cambio del nivel 200 al rango de valores 50-100), reflejan los pasos del actor en medio del escenario en la posición donde se encuentra localizado este píxel. Por razones computacionales, se asume que los valores de intensidad de los tres componentes en el espacio RGB (azul, verde y rojo) tienen la misma varianza \cite{zivkovic_efficient_2006}, de esta manera la matriz de covarianza queda definida como una matriz identidad multiplicado por ese único valor de varianza.

\begin{figure}[h!]
  \centering
      \includegraphics[scale=0.75]{img/histograma_pixel_340_160_3_2}
  \caption[Histograma secuencia completa ``\textit{Kick Camera 3 Person 4}'']{Histograma secuencia completa ``\textit{Kick Camera 3 Person 4}''}
\label{histograma}
\end{figure}

Al observar el rango valores asociados al escenario en la figura \ref{intensidad_340_160}, se evidencian valores más constantes (o menor dispersión) que los del actor cuando pasa por este píxel. Esto también se puede apreciar en el modelo de la secuencia completa que describe el histograma de la figura ~\ref{histograma}(a). En esta imagen se pueden distinguir dos curvas bien definidas, el fondo de imagen y el el actor en movimiento. La mixtura en este ejemplo, se construye ajustando la función de densidad a dos distribuciones Gaussianas, figura (\ref{histograma})(b). De esta manera, el modelo que describe este píxel en particular para esta secuencia, queda completamente definido por una composición de dos funciones Gaussianas. Se puede ver también en esta figura, que la imagen de fondo puede ser asociada con la componente de menor varianza, debido principalmente a la menor dispersión de valores del fondo. Sin embargo, el menor valor de frecuencia puede ser relacionado con el actor en movimiento, debido al menor tiempo que éste cruza el escenario. 

Se aprovecha estas características de la Mixtura de Gaussianas, para hacer una separación entre \textit{foreground} y \textit{background}. Así cuando la componente Gaussiana que describe algún píxel no se ajuste a la distribución con menor varianza, será considerando parte de un objeto en movimiento. No obstante, si este mismo objeto queda estático por un tiempo considerable, la varianza de su distribución Gaussiana comenzará a disminuir y tomar más relevancia (mayor ponderación relativa) que la varianza del componente del fondo, hasta llegar un momento que será considerado parte del fondo de imagen.




\begin{figure}[h!]
  \centering
      \includegraphics[scale=0.75]{img/scatter_pt_340_160_3D_Blue_Green_Red}
  \caption[Gráfico dispersión 3D secuencia completa ``\textit{Kick Camera 3 Person 4}'']{Gráfico de dispersión en secuencia ``\textit{Kick Camera 3 Person 4}'' que muestra la nube de puntos relacionados con el escenario y el actor en movimiento.}
\label{scatter_3D}
\end{figure}

\begin{figure}[h!]
  \centering
      \includegraphics[scale=0.75]{img/scatter_pt_340_160_isotropic_Blue_Green}
  \caption[Gráfico dispersión 2D secuencia completa ``\textit{Kick Camera 3 Person 4}'']{Gráfico de dispersión secuencia ``\textit{Kick Camera 3 Person 4}'', que muestra dos \textit{cluster} bien definidos}
\label{scatter_2D}
\end{figure}

Los gráficos de las figuras \ref{scatter_3D} y \ref{scatter_2D} sirven de ejemplo para entender el procedimiento de actualización de parámetros, de los diferentes componentes Gaussianos definidos en el algoritmo. Estas figuras son gráficos de dispersión de la secuencia completa (\textit{Kick Camera 3 Person 4}) para el mismo píxel del ejemplo señalado en la figura \ref{posicion_340_160}. La dispersión en la gráfica \ref{scatter_3D}, conforma principalmente dos nubes o \textit{clusters} que se asocian con los dos elementos principales de la secuencia: el escenario y las acciones del actor. El \textit{cluster} más compacto de puntos, localizado en el rango 150-250 de los tres ejes de colores, resume los diferentes tiempos que el píxel fue parte del escenario, mientras el otro \textit{cluster} localizado en la parte inferior refleja la cantidad de veces que el píxel fue parte del actor durante sus movimientos.  

Durante el tiempo que dura la ejecución del algoritmo, los componentes Gaussianos de un píxel van siendo creados o suprimidos en tiempo real, dependiendo exclusivamente de los nuevos valores de entrada y el tiempo de permanencia de estos valores, en alguno de los diferentes \textit{clusters} de la secuencia. Los valores presentan un comportamiento dinámico en directa relación con los eventos de la secuencia; éstos se pueden mover entre \textit{clusters} o bien mantenerse en uno de ellos por un largo periodo. Se aprovecha esta división de \textit{clusters} para hacer la separación entre \textit{background} y \textit{foreground}. La gráfica de la figura \ref{scatter_2D} (dispersión verde-azul) destaca cuatro componentes (\textit{clusters}) definidos por el algoritmo, en el instante que el actor se encuentra detenido de pie en medio del escenario, ejemplo de la figura \ref{posicion_340_160}(a). Los dos puntos en rojo revelan la posición del píxel en los \textit{clusters} para las dos imágenes de ejemplo en la figura \ref{posicion_340_160}. Cada uno de estos componentes tiene un factor de ponderación $\pi_k$ que indica el tiempo de permanencia del píxel en la nube de puntos. Este factor de ponderación es incrementado en la medida que el valor del píxel se mantenga dentro de un margen (de uno de los componentes), determinado por la distancia de Mahalanobis, del punto hacia cada uno de los \textit{clusters}. Para discriminar que el píxel es parte del actor (o el escenario), el algoritmo calcula las distancias del pixel a los ``centroides'' (media) de los diferentes \textit{clusters} establecidos en ese instante. El punto es asociado con algún \textit{cluster}, si el resultado de la distancia de Mahalanobis es menor a tres veces su varianza. Si el punto no es vinculado a ninguno de los clusters definidos en ese momento, el algoritmo crea uno nuevo y deja el punto ligado con este nuevo cluster. Una vez que el algoritmo ha identificado el punto en alguno de los clusters, se realiza la clasificación, ya sea \textit{background}  o \textit{foregorund} y por lo tanto el píxel queda clasificado por la característica del cluster, al que ha sido asociado. Esto es, si el cluster en particular tiene un factor de ponderación más alto que todos los demas, ese cluster es \textit{background} y en consecuencia el píxel también. En caso contrario, el píxel que pertenece a un cluster con factor de ponderación inferior, por lo tanto es algún objeto en movimiento.




\section{Mixtura de distribuciones Gaussianas - GMM}


La superposición de distribuciones Gaussianas que describen el comportamiento de un pixel, se define en la ecuación \eqref{eq:mixturegaussians}. Esta es una sumatoria de \textit{``K''} componentes Gaussianos $\mathcal{N}( x | \mu_k , \Sigma_k)$, cada uno ponderado por un factor multiplicativo $\pi_k$, que establece una función de densidad de probabilidad general para el píxel $p(x)$. El factor de ponderación $\pi_k$, es el grado de influencia de cada componente en el comportamiento global del píxel que esta siendo modelado. Los valores de ponderación pueden ser interpretados, como probabilidades de ocurrencias de las diferentes clases, \textit{Background} y \textit{Foreground}, representadas por los diferentes componentes Gaussianos; por ejemplo, $\pi_1$ podría representar la probabilidad que un píxel sea la imagen de fondo. La suma total de los factores $\pi_k$ es 1, indicado en la ecuación \eqref{eq:mixturefactor}. 

\begin{equation} \label{eq:mixturegaussians}
p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}( x | \mu_k , \Sigma_k)
\end{equation}

\begin{equation} \label{eq:mixturefactor}
\sum_{k=1}^{K} \pi_k = 1
\end{equation}


Una de las principales tareas del algoritmo que implementa mixtura de Gaussianas, es mantener actualizado los $K$ valores estimados de media ($\mu_1, ..., \mu_k$),  covarianza ($\Sigma_1, ..., \Sigma_k$) y el factor de ponderación ($\pi_1, ..., \pi_k$), por cada píxel de una nueva entrada. Se utilizan los valores de intensidad en el espacio de colores RGB, para construir la mixtura de componentes Gaussianas de un píxel.

La estimación de los parámetros $\vec{\theta} = \{\pi_1,..,\pi_k, \mu_1,..,\mu_k,\Sigma_1,..,\Sigma_k \} $ de las distintas mixturas se realiza mediante el método de Máxima Verosimilitud (\textit{Maximum Likelihood Estimate - MLE}). De esta manera, los parámetros estimados por máxima verosimilitud de un conjunto $t$ de muestras $\mathcal{X} = \{\vec{x}^{(1)}, ..., \vec{x}^{(t)}\}$, quedan determinados por la ecuación (~\ref{eq:maxima_verosimilutd}). Una explicación bien detallada de estimación mediante el método de máxima verosimilitud se puede ver en el capítulo 3 de la referencia \cite{duda_pattern_2000}.

\begin{equation} \label{eq:maxima_verosimilutd}
\hat{\vec{\theta}} = arg \max_{\vec{\theta}}  (\log p(\mathcal{X};\vec{\theta}))
\end{equation}

 
Debido a la dificultad de encontrar una solución analítica de \textit{MLE}, se emplea una aproximación numérica iterativa para maximizar la función de verosimilitud.  El algoritmo de esperanza-maximización (EM) \cite{dempster_maximum_1977}, se usa para encontrar soluciones de máxima verosimilitudes. Este es un procedimiento iterativo que busca un máximo local de la función log-verosimilitud. Este algoritmo es fácil de implementar, sin embargo una de sus limitaciones está en la posibilidad de converger en un máximo local no inicializado apropiadamente.


\subsection{Actualización parámetros del Algoritmo}

Esta parte, describe el método de actualización recursivo que se utiliza en la estimación de los parámetros ($\mu_k$, $\Sigma_k$, $\pi_k$) de cada componente Gaussiano, establecido por \textit{Zivkovic y Heidjen} \cite{zivkovic_efficient_2006}. Por razones computacionales, las matrices de covarianza son consideradas Isotrópicas, es decir la matriz de identidad multiplicado por un único valor de varianza.

El modelo del fondo, es estimado desde un conjunto ``$X$'' de entrenamiento, el cuál mantiene una historia específica de muestras por cada píxel. Para adecuarse a los posibles cambios que puedan producirse, las muestras en este conjunto se actualizan constantemente con nuevas entradas y simultáneamente las más antiguas son eliminadas. Un periodo de adaptación ``$T$'' (100 cuadros) es elegido para mantener la historia más reciente del fondo, y en el instante ``$t$'' se tiene el conjunto de entrenamiento $\mathcal{X_T} = \{x^{(t)}, ..., x^{(t-T)}\}$, desde el cual se estima la función de densidad del fondo.

La parte central del algoritmo, consiste en actualizar constantemente los parámetros $\vec{\theta}$ de las distribuciones Gaussianas, que describen el estado de cada píxel en un momento determinado. Las ecuaciones recursivas para estimar los parámetros de una muestra nueva $x^{(t)}$ en tiempo $t$ se presentan a continuación:


\begin{equation} \label{eq:mixturefactor_update}
\hat{\pi}_k \leftarrow  \hat{\pi}_k + \alpha(o^{(t)}_k - \hat{\pi}_k) - \alpha C_T
\end{equation}
\begin{equation} \label{eq:mixturemu_update}
\hat{\vec{\mu}}_k \leftarrow \hat{\vec{\mu}}_k + o^{(t)}_k (\alpha/\hat{\pi}_k) \vec{\delta}_k
\end{equation}
\begin{equation} \label{eq:mixturesigma_update}
\hat{\sigma}^2_k \leftarrow \hat{\sigma}^2_k + o^{(t)}_k (\alpha/\hat{\pi}_k) (\vec{\delta}^T_k \vec{\delta}_k - \hat{\sigma}^2_k)
\end{equation}
\begin{equation} \label{eq:mahalanobis_distance}
D^{2}_{k}(\vec{x}^{(t)})=\vec{\delta}^T\vec{\delta}_k/\hat{\sigma}^{2}_k
\end{equation}

\[
\vec{\delta}_k = \vec{x}^{(t)} - \hat{\vec{\mu}}_k
\]


El factor constante $\alpha$, define una curva de decaimiento exponencial para limitar la influencia de los datos más antiguos. Se usa esta constante, como reemplazo de $T$ mencionado anteriormente, relacionando ambas constante por la siguiente relación: $\alpha=1/T$. 

El algoritmo mantiene un conjunto dinámico de componentes por cada píxel, esto significa que el número de componentes del modelo es diferente en periodos distintos de la secuencia. Se determina que una nueva muestra es cercana con algunos de los componentes si la distancia de \textit{Mahalanobis} \eqref{eq:mahalanobis_distance} es por ejemplo, menor que tres. Fijando el valor del \textit{ownership} $o^{(t)}_k$ en uno si la muestra es cercana y en cero para el caso contrario. Si la muestra no es cercana a alguno de los componentes ($o^{(t)}_k=0$), se crea uno nuevo con los siguientes valores por defecto $\pi_{k+1} = \alpha$, $\hat{\vec{\mu}}_{k+1}=\vec{x}^{(t)}$ y $\hat{\sigma}_{k+1}=\sigma_0$, con $\sigma_0$ un valor de inicialización de la varianza fijado en el algoritmo.

Este algoritmo representa también los diferentes elementos que surgen en la secuencia, como un conjunto de \textit{clusters} en tiempo real. Un nuevo objeto que ingresa en la secuencia se caracteriza por un \textit{cluster} adicional con un valor de ponderación $\pi_k$ muy inferior a los otros componentes. 

Para determinar el píxel como una imagen de fondo, se ordena los factores de ponderación en forma descendente, y los $B$ \textit{clusters} con mayor ponderación se aproximan al modelo del fondo como se indica en la siguiente ecuación.

\begin{equation} \label{eq:clusters}
B=arg \max_{b}  (\sum_{k=1}^{b} \hat{\pi}_k > (1 - c_f))
\end{equation}

El parámetro $c_f$ es un criterio de medición que indica la cantidad de los datos que pueden ser considerados objetos de \textit{foreground} sin la influencia del modelo de \textit{background}. De esta forma si un nuevo objeto se mantiene estático durante algún tiempo, este será presentado como un \textit{cluster} adicional, y su factor de ponderación $\pi_{k+1}$ será incrementado en la medida que se mantenga estático. Sí se mantiene lo suficiente, y el factor $\pi_{k+1}$ supera $c_f$, entonces este objeto puede ser considerado parte del fondo.


\section{Modelo Mixtura de Gaussianas auto-adaptativo}

Este método se origina en un contexto de vigilancia de tráfico en ciudad. Es el resultado de un sistema de detección y clasificación automática de vehículos \cite{chen_vehicle_2012}, que utiliza una variante del método de mixtura de Gaussianas\cite{zivkovic_efficient_2006}, para hacer una separación entre vehículos en circulación y su pista. La modificación propuesta aborda el problema de cambios bruscos en la iluminación, que podrían transformar todo el segundo plano (\textit{background}) de la imagen, en primer plano (\textit{foreground}). Además, la tasa normal de aprendizaje se transforma en una tasa dinámica en tiempo real. Se intenta con esta modificación, obtener una tasa que se adapte en forma dinámica a los cambios de iluminación. También se utiliza como método de procesamiento previo, un filtro espacial y temporal\cite{chen_background_2009} para compensar las alteraciones producidas por vibraciones de las camaras. Finalmente para afrontar el problema de cambios en iluminación local, como sombras y reflexiones de luz, modifican un algoritmo de extracción de sombras \cite{horprasert_astatistical_1999}, para incorporar el factor de iluminación global.


\subsection{Factor de iluminación global}
En \cite{chen_self-adaptive_2011}, se menciona el modelo de detección de cambio invariante a iluminación (\textit{ICDM}), al proceso que determina el factor de cambio de iluminación global.  Éste es un procedimiento que realiza el cociente uno a uno entre un conjunto ``$s$'' de pixeles de una imagen de entrada ($i_c$) y su imagen de referencia ($i_r$). El factor de iluminación global ``$g$'' se determina como la mediana de todas las divisiones resultantes, ``\textit{Median of Quotient (MofQ)}''.

\begin{equation} \label{eq:mofq}
g = \median_{(s \in S)}  \left(\frac{i_{r,s}}{i_{c,s}}\right)
\end{equation}

Se define además un contador ``$c_k$'' por cada componente Gaussiano en la mixtura. Este contador se utiliza para mantener un registro de cuantos puntos han contribuido en la estimación de parámetros de cada Gaussiana, cada vez que los parámetros de un componente Gaussiano es actualizado, el contador también es incrementado. En caso que un nuevo componente se agrega a la mixtura este contador es inicializado a uno. 

El factor de aprendizaje $\alpha$ es usado como base para construir un nuevo factor de aprendizaje $\beta$, el cual utiliza el contador de componentes Gaussianas mencionado anteriormente. Las nuevas ecuaciones de actualización de parámetros se detallan a continuación:


\begin{equation} \label{eq:sagmm_betha}
\beta_k = \alpha(h + c_k)/c_k
\end{equation}

\begin{equation} \label{eq:sagmm_mu}
\hat{\vec{\mu}}_k \leftarrow \hat{\vec{\mu}}_k + o^{(t)}_k (\beta/\hat{\pi}_k) \vec{\delta}_k
\end{equation}

\begin{equation} \label{eq:sagmm_sigma}
\hat{\sigma}^2_k \leftarrow \hat{\sigma}^2_k + o^{(t)}_k (\beta/\hat{\pi}_k) (\vec{\delta}^T_k \vec{\delta}_k - \hat{\sigma}^2_k)
\end{equation}

\begin{equation} \label{eq:sagmm_ck}
c_k \leftarrow c_k + 1
\end{equation}

\begin{equation} \label{eq:sagmm_delta}
\hat{\vec{\delta}}_k = g \bullet \vec{x}^{t} - \hat{\vec{\mu}}_k
\end{equation}

Si el valor de un pixel de la imagen de fondo varia muy rápidamente, el valor del contador $c_k$ para ese píxel no será muy grande, por lo que el valor de $\beta$ aumentará. Actualizando de esta manera el fondo más rápidamente. Por otra parte si el fondo es muy estable, el valor se $\beta$ se aproximará a $\alpha$. También se observa que la distancia de Mahalanobis es balanceada por el factor de cambio de iluminación global $g$.


\subsection{Filtro espacial y temporal}
Este filtro que prepara la imagen antes de ser procesada por el método modificado de mixturas. Realiza un suavizado de cada componente espectral. En una imagen se habla de dominio espacial para referirse a los píxeles localizados en la matriz de valores y dominio espectral para los componentes de frecuencia obtenidos después de aplicar la transformada de Fourier. El domino espacial se relaciona con el dominio espectral, por medio del teorema de la \textit{convolución}. La convolución (descripción muy básica), es el proceso mover una mascara o kernel píxel a píxel sobre una imagen y obtener un resultado de cada píxel de acuerdo con los coeficientes de la mascara. Los filtros del tipo Gaussiano, tienen la particularidad que en ambos espacios (espacial y espectral) son representados por una función Gaussiana.


\begin{equation} \label{eq:sagmm_filter}
K_{h_s,h_s} = \frac{C}{h_s h_t} k \left( {\| \frac{x^s}{h_s} \|}^2 \right) k \left( {\| \frac{x^t}{h_t} \|}^2 \right)
\end{equation}

$C$ es una constante de normalización, $x^s$ es la parte espacial y $x^t$ la parte temporal, $k(x)$ es un perfil común de kernel (Gaussiano) usado en ambos dominios, temporal y espacial. Las variable $h_s$ y $h_t$ son el ancho de banda de los kernels. 


\begin{figure}[h!]
\centering
\fbox{\includegraphics[scale=0.5]{img/Spatio_Temporal_Filter_Example}}
\caption[Procesamiento temporal de tres imágenes]{Procesamiento temporal de tres imágenes de una secuencia.}
\label{fig:spatio_temporal_filter_example}
\end{figure}


La figura \ref{fig:spatio_temporal_filter_example} es un ejemplo del funcionamiento de este filtro. Se define una ventana temporal que define el número de imágenes que serán procesadas simultáneamente por este filtro. La entrada del filtro, es el número de imágenes que mantiene la ventana temporal, y su salida es la imagen filtrada que será la nueva entrada del algoritmo. Esta ventana puede ser visualizada como un marco que se desliza sobre la secuencia y entrega una imagen procesada como salida.

\begin{figure}[h!]
\centering
%\subfigure{\input{img/filter}}
\fbox{\includegraphics[scale=0.5]{img/Spatio_Temporal_Filter}}
\caption[Filtro temporal spacial]{Diagrama en bloques del filtro temporal y espacial. La parte de filtro temporal toma tres imágenes y aplica un filtro Gaussiano, el filtro espacial junta los componentes y aplica otro kernel Gaussiano.}
\label{fig:spatio_temporal_filter}
\end{figure}


Una representación en bloques de la implementación de este filtro se puede notar en la figura \ref{fig:spatio_temporal_filter}. La primera etapa es el procesamiento espacial, en que aplica un proceso de difuminado (\textit{blurring}) a cada uno de los colores de una imagen. Este procedimiento, es un suavizado Gaussiano que permite reducir el ruido en la imagen de cada uno de los componentes. La segunda etapa, consiste en agrupar por colores el resultado del filtrado espacial, para luego utilizar un segundo filtro Gaussiano, en cada uno de los tres componentes de las imágenes agrupadas por colores. El resultado es una suma de los tras componentes previamente filtrado. Finalmente, cada uno de los componentes se vuelven a juntar en una sola imagen con tres colores filtrados temporalmente.


La efectividad de este filtro puede ser comparado en los gráficos de la figura \ref{fig:result_spatio_temporal_filter}. La imagen \ref{fig:Figure_A} es el cluster de la imagen de fondo en funcionamiento normal del algoritmo sin filtro aplicado sobre las imágenes. En cambio la imagen de la figura \ref{fig:Figure_B} es el resultado final del cluster de la imagen de fondo,  aplicando procesamiento previo a las imágenes antes de ingresar en el algoritmo. Se puede observar que el cluster final filtrado es mucho más compacto que el otro. Finalmente esta de filtrado es una ayuda en el proceso de separación del fondo de imagen de los elementos móviles.

\begin{figure}
\centering     %%% not \center
\subfigure[Resultado final sin filtro]{\label{fig:Figure_A}\includegraphics[width=70mm]{img/cluster_bg_340_160}}
\subfigure[Resultado final con filtro]{\label{fig:Figure_B}\includegraphics[width=70mm]{img/cluster_bg_340_160_filter}}
\caption[Comparación de efectividad del filtro en una secuencia completa]{Comparación de efectividad del filtro Espacial-Temporal aplicado en la secuencia completa }
\label{fig:result_spatio_temporal_filter}
\end{figure}



